{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, sys, string, requests, csv\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from paper_filtering import filter_by_header\n",
    "from paper_filtering import filter_by_page_number_keep_missing\n",
    "from paper_filtering import filter_by_page_number_remove_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_keep_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_remove_missing\n",
    "from paper_filtering import filter_journals\n",
    "from paper_filtering import apply_filter_to_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eshost = \"130.56.248.215:9200\"\n",
    "\n",
    "threads = 40\n",
    "\n",
    "client = Elasticsearch(eshost, timeout=200, maxthreads = threads)\n",
    "\n",
    "\n",
    "# years to include note: range(inclusive,exclusive, i.e. range(1,3) = [1,2]\n",
    "yearrange = range(2007,2020)\n",
    "\n",
    "# filepath to the file containing venue names and their categories (conf or journal)\n",
    "venue_category_filename = os.path.join(os.pardir,\"app\",\"data\",\"venue_list.csv\")\n",
    "\n",
    "\n",
    "dblp_raw_filename = lambda name,year: os.path.join( \"DBLP_raw_data\" , \n",
    "                                              \"{}_{}_raw_dblp_papers.json\".format(name,year) )\n",
    "\n",
    "filtered_papers_filename = lambda name,year: os.path.join( \"filtered_papers\" , \n",
    "                                              \"{}_{}_filtered_papers.json\".format(name,year) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_academic_search(type, url, query):\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Ocp-Apim-Subscription-Key': '4698d5e7b0244e828d1dc21134238650',    # bens\n",
    "    }\n",
    "    if type == \"get\":\n",
    "        response = requests.get(url, params=urllib.parse.urlencode(query), headers=headers)\n",
    "    elif type == \"post\":\n",
    "        response = requests.post(url, json=query, headers=headers)\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"return statue: \" + str(response.status_code))\n",
    "#         print(\"ERROR: problem with the request.\")\n",
    "#         print(response.content)\n",
    "        #exit()\n",
    "    return json.loads((response.content).decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def interpret_title(title):\n",
    "    \n",
    "        \n",
    "    MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "    url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/interpret\")\n",
    "    query = {\n",
    "      \"query\": title,\n",
    "      \"count\": 1,\n",
    "      \"offset\": 0,\n",
    "      \"attributes\": \"Ti\"\n",
    "    }\n",
    "\n",
    "    data = query_academic_search(\"get\", url, query)\n",
    "    expr = data[\"interpretations\"][0][\"rules\"][0][\"output\"][\"value\"]\n",
    "    \n",
    "    return expr\n",
    "\n",
    "\n",
    "def evaluate_expr(query):\n",
    "    \n",
    "    \n",
    "    MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "    url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/evaluate\")\n",
    "    query = {\n",
    "      \"expr\": \"Ti='{}'\".format(query),\n",
    "      \"count\": 20,\n",
    "      \"offset\": 0,\n",
    "      \"attributes\": \"Id,Ti,Y,AA.AuId,AA.AfId,CC,ECC\"\n",
    "    }\n",
    "\n",
    "    data = query_academic_search(\"get\", url, query)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "def get_title_from_MAG(title):\n",
    "    \n",
    "    try:\n",
    "        expr = interpret_title(title)\n",
    "        mag_title = expr[4:-1]\n",
    "        \n",
    "    except:\n",
    "        mag_title = None\n",
    "\n",
    "    return mag_title\n",
    "\n",
    "\n",
    "def canonical(title):\n",
    "    title = title.lower()\n",
    "    title = title.translate(str.maketrans(punctuation, \" \"*len(punctuation)))\n",
    "    title = \" \".join(title.split())\n",
    "\n",
    "\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_es_canonical_title(papers):\n",
    "\n",
    "    year = papers[0][\"year\"]\n",
    "    \n",
    "    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\n",
    "        \n",
    "        \n",
    "    batches = divide_batches(set(papers.keys()),20)\n",
    "\n",
    "    \n",
    "    for batch in batches:\n",
    "        \n",
    "        canonical_titles = {canonical(title) for title in batch}\n",
    "\n",
    "        search = Search(index = \"papers*\", using = client)\n",
    "        query = {\"query\":{\n",
    "            \"bool\":{\n",
    "                \"should\": [{\"match_phrase\": {\"PaperTitle\": ct}} for ct in canonical_titles]\n",
    "            }\n",
    "        }}\n",
    "\n",
    "        search.update_from_dict(query)\n",
    "\n",
    "        source_fields = ['PaperId',\"PaperTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "        search = search.source(source_fields)\n",
    "\n",
    "        for res in search.scan():\n",
    "            if \"Year\" in res and res[\"Year\"] == year and res[\"PaperTitle\"] in canonical_titles:\n",
    "                ({field: res[field] for field in source_fields})\n",
    "\n",
    "                papers[res[\"PaperTitle\"]][\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "                papers[res[\"PaperTitle\"]][\"source\"] = \"ES\"\n",
    "\n",
    "    return list(papers.values())\n",
    "\n",
    "    \n",
    "def get_info_es_display_title(paper):\n",
    "    search = Search(index = \"papers*\", using = client)\n",
    "    query = {\"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\": {\"match_phrase\": {\"OriginalTitle\": paper[\"DBLP title\"]}}\n",
    "        }\n",
    "    }}\n",
    "\n",
    "    search.update_from_dict(query)\n",
    "\n",
    "    source_fields = ['PaperId',\"PaperTitle\", \"OriginalTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "    search = search.source(source_fields)\n",
    "\n",
    "    for res in search.scan():\n",
    "        if \"Year\" in res and res[\"Year\"]==paper[\"year\"] and res[\"OriginalTitle\"] == paper[\"DBLP title\"]:\n",
    "            paper[\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "            \n",
    "            paper[\"source\"] = \"ES\"\n",
    "\n",
    "    return paper\n",
    "    \n",
    "\n",
    "def get_info_MAG(paper):\n",
    "    \n",
    "    if \"source\" in paper:\n",
    "        return paper\n",
    "    \n",
    "    title = get_title_from_MAG(paper[\"DBLP title\"])\n",
    "    \n",
    "    if title == None:\n",
    "        return paper\n",
    "\n",
    "    paper_data = evaluate_expr(title)\n",
    "    \n",
    "    key_pairs = [\n",
    "        (\"PaperTitle\",\"Ti\"),\n",
    "        (\"PaperId\",\"Id\"),\n",
    "        (\"Year\",\"Y\"),\n",
    "        (\"CitationCount\",\"CC\"),\n",
    "        (\"EstimatedCitation\", \"ECC\")\n",
    "    ]\n",
    "    \n",
    "    if \"entities\" not in paper_data:\n",
    "        return paper\n",
    "    \n",
    "    for entity in paper_data[\"entities\"]:\n",
    "        info = {tokey: entity[fromkey] for tokey, fromkey in key_pairs}\n",
    "        info[\"Affiliations\"] = [None if \"AfId\" not in author else author[\"AfId\"] for author in entity[\"AA\"]]\n",
    "        info[\"Authors\"] = [None if \"AuId\" not in author else author[\"AuId\"] for author in entity[\"AA\"]]\n",
    "        paper[\"MAG papers\"].append(info)\n",
    "    \n",
    "    if len(paper[\"MAG papers\"]) > 0:\n",
    "        paper[\"source\"] = \"MAG\"\n",
    "        return paper\n",
    "    \n",
    "    return paper\n",
    "\n",
    "    \n",
    "def get_paper_info(papers):    \n",
    "    \n",
    "    papers = get_info_es_canonical_title(papers)\n",
    "    \n",
    "    for paper in papers:\n",
    "        if \"source\" not in paper:\n",
    "            paper = get_info_es_display_title(paper)\n",
    "\n",
    "        if \"source\" not in paper:\n",
    "            paper = get_info_MAG(paper)\n",
    "\n",
    "        if \"source\" not in paper:\n",
    "            paper[\"source\"] = None\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "def get_paper_affiliations(papers):\n",
    "    \n",
    "    affiliations = set()\n",
    "    \n",
    "    mag_dict = dict()\n",
    "    \n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            if \"Affiliations\" not in mag_paper:\n",
    "                mag_dict[mag_paper[\"PaperId\"]] = mag_paper[\"PaperTitle\"]\n",
    "                mag_paper[\"Affiliations\"] = list()\n",
    "                \n",
    "    \n",
    "    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\n",
    "\n",
    "    \n",
    "    batches = divide_batches(set(mag_dict.keys()),200)\n",
    "    \n",
    "    for batch in batches:\n",
    "\n",
    "        search = Search(index = \"paperauthoraffiliations*\", using = client)\n",
    "        query = { \"query\": { \n",
    "            \"bool\": {\n",
    "                \"should\": [{\"term\": {\"PaperId\": pid}} for pid in batch]\n",
    "            } \n",
    "        }  }\n",
    "        search.update_from_dict(query)\n",
    "        search = search.source(['PaperId','AuthorId','AffiliationId'])\n",
    "\n",
    "\n",
    "        for res in search.scan():\n",
    "            \n",
    "            title = mag_dict[res[\"PaperId\"]]\n",
    "            try:\n",
    "                for mag_paper in papers[title][\"MAG papers\"]:\n",
    "                    if mag_paper[\"PaperId\"] == res[\"PaperId\"]:\n",
    "                        if \"AffiliationId\" in res:\n",
    "                            mag_paper[\"Affiliations\"].append(res[\"AffiliationId\"])\n",
    "                            affiliations.add(res[\"AffiliationId\"])\n",
    "                        else:\n",
    "                            mag_paper[\"Affiliations\"].append(None)\n",
    "            except:\n",
    "                for paper in papers.values():\n",
    "                    for mag_paper in paper[\"MAG papers\"]:\n",
    "                        if mag_paper[\"PaperId\"] == res[\"PaperId\"]:\n",
    "                            if \"AffiliationId\" in res:\n",
    "                                mag_paper[\"Affiliations\"].append(res[\"AffiliationId\"])\n",
    "                                affiliations.add(res[\"AffiliationId\"])\n",
    "                            else:\n",
    "                                mag_paper[\"Affiliations\"].append(None)\n",
    "\n",
    "    papers = list(papers.values())\n",
    "    \n",
    "    return papers, affiliations\n",
    "\n",
    "\n",
    "def divide_batches(list_like,n):\n",
    "    \n",
    "    list_like = list(list_like)\n",
    "    size = len(list_like)\n",
    "    \n",
    "    return [list_like[0+(n*x):min(n*(x+1),size)] for x in range(int(np.ceil(size/n)))]\n",
    "    \n",
    "\n",
    "def link_papers_with_affiliation_names(papers,affiliationids):\n",
    "\n",
    "    \n",
    "    aff_id_batches = divide_batches(affiliationids,100)\n",
    "    \n",
    "    affiliations = dict()\n",
    "    \n",
    "    # get affiliation names\n",
    "    for batch in aff_id_batches:\n",
    "    \n",
    "        search = Search(index = \"affiliations\", using = client)\n",
    "        query = {\"query\":{\n",
    "            \"bool\":{\n",
    "                \"should\": [{\"match\": {\"AffiliationId\": aid}} for aid in batch]\n",
    "            }\n",
    "        }}\n",
    "\n",
    "        search.update_from_dict(query)\n",
    "\n",
    "        source_fields = ['AffiliationId',\"NormalizedName\"]\n",
    "\n",
    "        search = search.source(source_fields)\n",
    "\n",
    "        try:\n",
    "            for res in search.scan():\n",
    "                affiliations[res[\"AffiliationId\"]] = res[\"NormalizedName\"]\n",
    "        except:\n",
    "            print(affiliationids)\n",
    "            print(query)\n",
    "            print(1[2])\n",
    "        \n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_paper[\"Affiliations\"] = [None if affiliation not in affiliations \n",
    "                                         else affiliations[affiliation] \n",
    "                                         for affiliation in mag_paper[\"Affiliations\"]]\n",
    "\n",
    "    \n",
    "    return papers\n",
    "    \n",
    "\n",
    "def get_information_for_venue_papers(venue, venuetype, yearrange=yearrange, force=False):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "    \n",
    "    \n",
    "    for year in yearrange:\n",
    "\n",
    "        in_filename = dblp_raw_filename(venue,year)\n",
    "        out_filename = filtered_papers_filename(venue,year)\n",
    "\n",
    "        \n",
    "        # check whether the file already exists\n",
    "        if os.path.exists(out_filename) and not force:\n",
    "            continue\n",
    "\n",
    "        with open(in_filename, \"r\") as fh:\n",
    "            papers = json.load(fh)\n",
    "\n",
    "        papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "\n",
    "        if len(papers) == 0:\n",
    "            with open(out_filename,\"w\") as fh:\n",
    "                json.dump([],fh)\n",
    "            continue\n",
    "        \n",
    "        affiliation_ids = set()\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for row in papers:\n",
    "\n",
    "            paper = dict()\n",
    "\n",
    "            paper[\"DBLP title\"] = row[\"title\"]\n",
    "            paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "            paper[\"year\"] = row[\"year\"]\n",
    "            paper[\"MAG papers\"] = list()\n",
    "            \n",
    "            output.append(paper)\n",
    "\n",
    "        output = get_paper_info(output)\n",
    "\n",
    "        output, paper_affiliations = get_paper_affiliations(output)\n",
    "\n",
    "        affiliation_ids.update(paper_affiliations)\n",
    "            \n",
    "        output = link_papers_with_affiliation_names(output, affiliation_ids)\n",
    "        \n",
    "        with open(out_filename,\"w\") as fh:\n",
    "            json.dump(output,fh)\n",
    "            \n",
    "    \n",
    "    print(venue)\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pools(task, lists, agg_f=None):\n",
    "    pool = Pool(processes = threads)\n",
    "    result = []\n",
    "    for x in lists:\n",
    "        result.append(pool.apply_async(task,(x,)))\n",
    "    \n",
    "    if agg_f is None:\n",
    "        def agg_f(x):\n",
    "            pass\n",
    "    for rs in result:\n",
    "        agg_f(rs.get())\n",
    "    pool.close()\n",
    "    \n",
    "    \n",
    "def popn(xs,n):\n",
    "    popped = list()\n",
    "    for i in range(n):\n",
    "        if len(xs) == 0:\n",
    "            break\n",
    "        popped.append(xs.pop())\n",
    "    return popped\n",
    "            \n",
    "def get_pool_lists(ls, threads):\n",
    "    ls_ = ls.copy()\n",
    "    if type(ls_) != list:\n",
    "        ls_ = list(ls_)\n",
    "    pool_lists = list()\n",
    "    list_size = len(ls) // threads\n",
    "    for i in range(threads-1):\n",
    "        pool_lists.append(popn(ls_,list_size))\n",
    "    pool_lists.append(ls_)\n",
    "    return pool_lists\n",
    "\n",
    "def task(venues):\n",
    "    \n",
    "    for venue, venuetype in venues:\n",
    "        get_information_for_venue_papers(venue, venuetype)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trets\n",
      "wine\n",
      "stacs\n",
      "uist\n",
      "sigcomm\n",
      "tist\n",
      "talip\n",
      "tocs\n",
      "tomccap\n",
      "ssdbm\n",
      "software\n",
      "siamcomp\n",
      "wcci\n",
      "tissec\n",
      "rtas\n",
      "tpds\n",
      "talg\n",
      "tomacs\n",
      "siamsc\n",
      "ubicomp\n",
      "ssd\n",
      "toit\n",
      "tosn\n",
      "taco\n",
      "siamrev\n",
      "popl\n",
      "tocl\n",
      "wads\n",
      "soda\n",
      "sea\n",
      "tecs\n",
      "siamco\n",
      "pact\n",
      "naacl\n",
      "rss\n",
      "lats\n",
      "jea\n",
      "tois\n",
      "micro\n",
      "uai\n",
      "taccess\n",
      "iticse\n",
      "tosem\n",
      "sigsoft-fse\n",
      "sdm\n",
      "pods\n",
      "ispa\n",
      "vr\n",
      "tochi\n",
      "icws\n",
      "interact\n",
      "srds\n",
      "iros\n",
      "siamam\n",
      "siamnum\n",
      "icnp\n",
      "tdsc\n",
      "icebe\n",
      "osdi\n",
      "kr\n",
      "recomb\n",
      "mobisys\n",
      "tog\n",
      "icc\n",
      "jdiq\n",
      "tweb\n",
      "tacas\n",
      "miccai\n",
      "tos\n",
      "itc\n",
      "eurosys\n",
      "sc\n",
      "tmm\n",
      "edcc\n",
      "sigmod\n",
      "hpcc\n",
      "vldb\n",
      "podc\n",
      "isorc\n",
      "gecco\n",
      "infocom\n",
      "siammax\n",
      "ddecs\n",
      "cse\n",
      "cga\n",
      "icmr\n",
      "colt\n",
      "siamads\n",
      "icst\n",
      "cal\n",
      "ipsn\n",
      "3dim\n",
      "icdt\n",
      "asiacrypt\n",
      "spin\n",
      "oopsla\n",
      "tcc\n",
      "re\n",
      "kdd\n",
      "tods\n",
      "mobisec\n",
      "icassp\n",
      "tvlsi\n",
      "jcdl\n",
      "taas\n",
      "mdm\n",
      "iswc\n",
      "toplas\n",
      "sbac-pad\n",
      "tmc\n",
      "eurocrypt\n",
      "sigmetrics\n",
      "edbt\n",
      "hpca\n",
      "vee\n",
      "pldi\n",
      "siamma\n",
      "fse\n",
      "dcoss\n",
      "ccs\n",
      "imc\n",
      "ismvl\n",
      "icml\n",
      "cscw\n",
      "coling\n",
      "icsm\n",
      "si3d\n",
      "cacm\n",
      "ipmi\n",
      "icdm\n",
      "ase\n",
      "nsdi\n",
      "aaai\n",
      "spaa\n",
      "tcbb\n",
      "raid\n",
      "jocch\n",
      "todaes\n",
      "mobihoc\n",
      "tvcg\n",
      "icaps\n",
      "sysose\n",
      "saint\n",
      "mass\n",
      "issta\n",
      "jair\n",
      "ton\n",
      "tkde\n",
      "esorics\n",
      "sigir\n",
      "ecoop\n",
      "uss\n",
      "hotos\n",
      "pervasive\n",
      "focs\n",
      "date\n",
      "ccgrid\n",
      "siamjo\n",
      "ijcar\n",
      "crypto\n",
      "iclp\n",
      "ismm\n",
      "cluster\n",
      "icse\n",
      "sensys\n",
      "bpm\n",
      "ipdps\n",
      "icde\n",
      "sp\n",
      "noms\n",
      "tc\n",
      "aamas\n",
      "pvldb\n",
      "asap\n",
      "mobicom\n",
      "toct\n",
      "tslp\n",
      "jmlr\n",
      "stoc\n",
      "icalp\n",
      "sacmat\n",
      "issre\n",
      "mascots\n",
      "jacm\n",
      "tkdd\n",
      "toms\n",
      "usenix\n",
      "hoti\n",
      "percom\n",
      "esop\n",
      "eccv\n",
      "sigecom\n",
      "fmcad\n",
      "cccg\n",
      "siamdm\n",
      "datamine\n",
      "icis\n",
      "ijcai\n",
      "cp\n",
      "ics\n",
      "civr\n",
      "ismb\n",
      "securecomm\n",
      "bioinformatics\n",
      "ipccc\n",
      "icdcs\n",
      "sosp\n",
      "nips\n",
      "tap\n",
      "acl\n",
      "arith\n",
      "psb\n",
      "mmas\n",
      "tse\n",
      "icac\n",
      "jmicro\n",
      "sac\n",
      "isscc\n",
      "iui\n",
      "lics\n",
      "titb\n",
      "pami\n",
      "ecai\n",
      "hipeac\n",
      "esa\n",
      "sigcse\n",
      "fccm\n",
      "ccc\n",
      "conext\n",
      "icgse\n",
      "iiswc\n",
      "dac\n",
      "icra\n",
      "bibe\n",
      "sec\n",
      "cikm\n",
      "ismar\n",
      "iolts\n",
      "iccv\n",
      "nfm\n",
      "acsac\n",
      "mm\n",
      "iaai\n",
      "ppopp\n",
      "apscc\n",
      "rtss\n",
      "jetc\n",
      "issac\n",
      "lctes\n",
      "itpro\n",
      "pacmpl\n",
      "hipc\n",
      "fast\n",
      "dt\n",
      "er\n",
      "computer\n",
      "cc\n",
      "icfp\n",
      "ieeemm\n",
      "icpr\n",
      "cvpr\n",
      "chi\n",
      "islped\n",
      "icccn\n",
      "avss\n",
      "interspeech\n",
      "ai\n",
      "ndss\n",
      "ht\n",
      "ml\n",
      "annals\n",
      "jeric\n",
      "ispass\n",
      "lcn\n",
      "fase\n",
      "itng\n",
      "grid\n",
      "disc\n",
      "compsac\n",
      "cav\n",
      "emsoft\n",
      "icer\n",
      "icwsm\n",
      "icpp\n",
      "csur\n",
      "isca\n",
      "cgo\n",
      "ancs\n",
      "internet\n",
      "expert\n",
      "iccad\n",
      "digitel\n",
      "ats\n",
      "emnlp\n",
      "aiccsa\n",
      "compgeom\n",
      "hpdc\n",
      "gis\n",
      "cases\n",
      "csfw\n",
      "cgf\n",
      "aim\n",
      "asplos\n",
      "amai\n",
      "aina\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'canonical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/minjeongshin/miniconda2/envs/csmetrics/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-8-bc3b56fe17da>\", line 37, in task\n    get_information_for_venue_papers(venue, venuetype)\n  File \"<ipython-input-7-9ed07903be9d>\", line 259, in get_information_for_venue_papers\n    output = get_paper_info(output)\n  File \"<ipython-input-7-9ed07903be9d>\", line 99, in get_paper_info\n    papers = get_info_es_canonical_title(papers)\n  File \"<ipython-input-7-9ed07903be9d>\", line 5, in get_info_es_canonical_title\n    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\n  File \"<ipython-input-7-9ed07903be9d>\", line 5, in <dictcomp>\n    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\nNameError: name 'canonical' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2c088f12c3e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpool_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pool_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvenues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrun_pools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_lists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bc3b56fe17da>\u001b[0m in \u001b[0;36mrun_pools\u001b[0;34m(task, lists, agg_f)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0magg_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/csmetrics/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'canonical' is not defined"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "venues = list()\n",
    "\n",
    "completed = list()\n",
    "\n",
    "with open(venue_category_filename, \"r\") as fh:\n",
    "    \n",
    "    reader = csv.reader(fh, delimiter=\",\")\n",
    "    \n",
    "    # skip header row\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        venue_type = row[4]   \n",
    "        name = row[0]\n",
    "    \n",
    "        venues.append((name, venue_type))\n",
    "\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to add additional filtered papers to a venue,year pair that has been affected by a change in the raw papers scraped or the filtering system without having to regather the information for the exisitng papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_for_venue_papers_add_additional_papers(venue, venuetype, year):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "\n",
    "    in_filename = dblp_raw_filename(venue,year)\n",
    "    out_filename = filtered_papers_filename(venue,year)\n",
    "    \n",
    "    \n",
    "    with open(in_filename, \"r\") as fh:\n",
    "        papers = json.load(fh)\n",
    "\n",
    "\n",
    "    papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "    \n",
    "    \n",
    "    with open(out_filename, \"r\") as fh:\n",
    "        output = json.load(fh)\n",
    "\n",
    "    original_output_size = len(output)\n",
    "    \n",
    "    existing_papers = [(paper[\"DBLP title\"],paper[\"year\"]) for paper in output]\n",
    "    \n",
    "    additional_papers = [paper for paper in papers if (paper[\"title\"],paper[\"year\"]) not in existing_papers]\n",
    "    \n",
    "    if len(additional_papers) == 0:\n",
    "        print(venue,year,\"nothing to add\")\n",
    "        return\n",
    "\n",
    "    affiliation_ids = set()\n",
    "    \n",
    "    additional_output = list()\n",
    "    \n",
    "    for row in additional_papers:\n",
    "\n",
    "        paper = dict()\n",
    "\n",
    "        paper[\"DBLP title\"] = row[\"title\"]\n",
    "        paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "        paper[\"year\"] = row[\"year\"]\n",
    "        paper[\"MAG papers\"] = list()\n",
    "\n",
    "        additional_output.append(paper)\n",
    "\n",
    "    additional_output = get_paper_info(additional_output)\n",
    "\n",
    "    additional_output, paper_affiliations = get_paper_affiliations(additional_output)\n",
    "\n",
    "    affiliation_ids.update(paper_affiliations)\n",
    "\n",
    "    additional_output = link_papers_with_affiliation_names(additional_output, affiliation_ids)\n",
    "\n",
    "    output.extend(additional_output)\n",
    "    \n",
    "    final_output_size = len(output)\n",
    "\n",
    "    with open(out_filename,\"w\") as fh:\n",
    "        json.dump(output,fh)\n",
    "        \n",
    "    print(venue,year,\"from\",original_output_size,\"to\",final_output_size)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wimob 2007 nothing to add\n",
      "sigsoft-fse 2019 nothing to add\n",
      "iticse 2009 nothing to add\n",
      "podc 2008 nothing to add\n",
      "sigcse 2015 nothing to add\n",
      "mobisys 2014 nothing to add\n",
      "mobisys 2017 nothing to add\n",
      "mobisys 2015 nothing to add\n",
      "tocs 2007 from 1 to 11\n",
      "tap 2007 from 0 to 17\n",
      "jeric 2007 from 0 to 6\n",
      "tosn 2007 from 0 to 22\n",
      "toit 2007 from 0 to 26\n",
      "toms 2007 from 1 to 28\n",
      "sigmetrics 2017 from 0 to 29\n",
      "sigcse 2018 nothing to add\n",
      "securecomm 2008 from 0 to 36\n",
      "mobihoc 2011 from 0 to 25\n",
      "talip 2007 from 3 to 14\n",
      "tois 2013 from 11 to 22\n",
      "tosem 2007 from 7 to 19\n",
      "jacm 2007 from 4 to 32\n",
      "iticse 2011 nothing to add\n",
      "tomccap 2007 from 8 to 26\n",
      "jetc 2007 from 0 to 13\n",
      "tomacs 2007 from 4 to 22\n",
      "ismvl 2007 from 0 to 58\n",
      "tois 2007 from 0 to 25\n",
      "tocl 2007 from 0 to 31\n",
      "toplas 2007 from 1 to 43\n",
      "sc 2008 from 0 to 65\n",
      "talg 2007 from 9 to 50\n",
      "tochi 2007 from 0 to 14\n",
      "tog 2011 from 40 to 190\n",
      "sc 2012 from 0 to 105\n",
      "icpp 2007 from 0 to 77\n",
      "tog 2007 from 0 to 128\n",
      "sc 2007 from 0 to 58\n",
      "saint 2007 from 0 to 18\n",
      "tods 2007 from 0 to 29\n",
      "todaes 2007 from 24 to 50\n",
      "icis 2007 from 0 to 158\n",
      "aamas 2007 from 0 to 263\n",
      "tog 2009 from 37 to 185\n",
      "ancs 2010 from 0 to 37\n",
      "icdcs 2007 from 0 to 70\n",
      "cgo 2014 from 0 to 29\n",
      "icis 2010 from 0 to 267\n",
      "iccad 2018 from 3 to 135\n",
      "tog 2008 from 17 to 166\n",
      "icis 2009 from 0 to 205\n",
      "icis 2008 from 0 to 215\n",
      "chi 2018 from 0 to 665\n",
      "tweb 2007 from 0 to 14\n",
      "tkdd 2007 from 0 to 13\n",
      "taco 2007 from 0 to 19\n",
      "tslp 2007 from 6 to 12\n",
      "taas 2007 from 0 to 14\n",
      "tissec 2007 from 0 to 11\n",
      "tecs 2007 from 7 to 37\n",
      "ssdbm 2007 from 0 to 34\n",
      "chi 2019 from 0 to 702\n",
      "compgeom 2014 from 1 to 70\n",
      "conext 2007 from 0 to 28\n",
      "conext 2008 from 0 to 77\n",
      "conext 2010 from 0 to 28\n",
      "conext 2011 from 0 to 30\n",
      "csur 2007 from 0 to 12\n",
      "emsoft 2018 from 0 to 15\n",
      "fast 2012 from 0 to 26\n",
      "gis 2007 from 0 to 66\n",
      "gis 2008 from 0 to 75\n",
      "icac 2007 from 0 to 32\n",
      "iccad 2016 from 0 to 133\n",
      "844.786933\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "threads = 20\n",
    "\n",
    "def additional_papers_task(venues):\n",
    "    \n",
    "    for venue, venuetype, year in venues:\n",
    "        get_information_for_venue_papers_add_additional_papers(venue, venuetype, year)\n",
    "    \n",
    "    return None\n",
    "\n",
    "venues = list()\n",
    "\n",
    "with open(\"single_page_number_exclusions.csv\",\"r\") as fh:\n",
    "    reader = csv.reader(fh,delimiter=\",\")\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        year = int(row[1])\n",
    "        venue_type = row[-1]\n",
    "        venues.append((key,venue_type,year))\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(additional_papers_task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
