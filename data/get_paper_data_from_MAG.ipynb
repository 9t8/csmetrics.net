{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, sys, string, requests, csv\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from paper_filtering import filter_by_header\n",
    "from paper_filtering import filter_by_page_number_keep_missing\n",
    "from paper_filtering import filter_by_page_number_remove_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_keep_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_remove_missing\n",
    "from paper_filtering import filter_journals\n",
    "from paper_filtering import apply_filter_to_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eshost = \"localhost:9200\"\n",
    "\n",
    "threads = 40\n",
    "\n",
    "client = Elasticsearch(eshost, timeout=200, maxthreads = threads)\n",
    "\n",
    "# years to include note: range(inclusive,exclusive, i.e. range(1,3) = [1,2]\n",
    "yearrange = range(2007,2021)\n",
    "\n",
    "# filepath to the file containing venue names and their categories (conf or journal)\n",
    "venue_category_filename = os.path.join(os.pardir,\"app\",\"data\",\"venue_list.csv\")\n",
    "\n",
    "\n",
    "dblp_raw_filename = lambda name,year: os.path.join( \"DBLP_raw_data\" , \n",
    "                                              \"{}_{}_raw_dblp_papers.json\".format(name,year) )\n",
    "\n",
    "filtered_papers_filename = lambda name,year: os.path.join( \"filtered_papers\" , \n",
    "                                              \"{}_{}_filtered_papers.json\".format(name,year) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLICATED (2021 June) -- search data from ES only (not from academic search API)\n",
    "#\n",
    "#\n",
    "# def query_academic_search(type, url, query):\n",
    "#     headers = {\n",
    "#         # Request headers\n",
    "#         'Ocp-Apim-Subscription-Key': '4698d5e7b0244e828d1dc21134238650',    # bens\n",
    "#     }\n",
    "#     if type == \"get\":\n",
    "#         response = requests.get(url, params=urllib.parse.urlencode(query), headers=headers)\n",
    "#     elif type == \"post\":\n",
    "#         response = requests.post(url, json=query, headers=headers)\n",
    "# #     if response.status_code != 200:\n",
    "# #         print(\"return statue: \" + str(response.status_code))\n",
    "# #         print(\"ERROR: problem with the request.\")\n",
    "# #         print(response.content)\n",
    "#         #exit()\n",
    "#     return json.loads((response.content).decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# def interpret_title(title):\n",
    "    \n",
    "        \n",
    "#     MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "#     url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/interpret\")\n",
    "#     query = {\n",
    "#       \"query\": title,\n",
    "#       \"count\": 1,\n",
    "#       \"offset\": 0,\n",
    "#       \"attributes\": \"Ti\"\n",
    "#     }\n",
    "\n",
    "#     data = query_academic_search(\"get\", url, query)\n",
    "#     expr = data[\"interpretations\"][0][\"rules\"][0][\"output\"][\"value\"]\n",
    "    \n",
    "#     return expr\n",
    "\n",
    "\n",
    "# def evaluate_expr(query):\n",
    "    \n",
    "    \n",
    "#     MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "#     url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/evaluate\")\n",
    "#     query = {\n",
    "#       \"expr\": \"Ti='{}'\".format(query),\n",
    "#       \"count\": 20,\n",
    "#       \"offset\": 0,\n",
    "#       \"attributes\": \"Id,Ti,Y,AA.AuId,AA.AfId,CC,ECC\"\n",
    "#     }\n",
    "\n",
    "#     data = query_academic_search(\"get\", url, query)\n",
    "    \n",
    "#     return data\n",
    "    \n",
    "    \n",
    "# def get_title_from_MAG(title):\n",
    "    \n",
    "#     try:\n",
    "#         expr = interpret_title(title)\n",
    "#         mag_title = expr[4:-1]\n",
    "        \n",
    "#     except:\n",
    "#         mag_title = None\n",
    "\n",
    "#     return mag_title\n",
    "\n",
    "\n",
    "def canonical(title):\n",
    "    title = title.lower()\n",
    "    title = title.translate(str.maketrans(punctuation, \" \"*len(punctuation)))\n",
    "    title = \" \".join(title.split())\n",
    "\n",
    "\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_es_canonical_title(papers):\n",
    "\n",
    "    year = papers[0][\"year\"]\n",
    "    \n",
    "    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\n",
    "        \n",
    "        \n",
    "    batches = divide_batches(set(papers.keys()),20)\n",
    "\n",
    "    \n",
    "    for batch in batches:\n",
    "        \n",
    "        canonical_titles = {canonical(title) for title in batch}\n",
    "\n",
    "        search = Search(index = \"papers*\", using = client)\n",
    "        query = {\"query\":{\n",
    "            \"bool\":{\n",
    "                \"should\": [{\"match_phrase\": {\"PaperTitle\": ct}} for ct in canonical_titles]\n",
    "            }\n",
    "        }}\n",
    "\n",
    "        search.update_from_dict(query)\n",
    "\n",
    "        source_fields = ['PaperId',\"PaperTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "        search = search.source(source_fields)\n",
    "\n",
    "        for res in search.scan():\n",
    "            if \"Year\" in res and res[\"Year\"] == year and res[\"PaperTitle\"] in canonical_titles:\n",
    "                ({field: res[field] for field in source_fields})\n",
    "\n",
    "                papers[res[\"PaperTitle\"]][\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "                papers[res[\"PaperTitle\"]][\"source\"] = \"ES\"\n",
    "\n",
    "    return list(papers.values())\n",
    "\n",
    "    \n",
    "def get_info_es_display_title(paper):\n",
    "    search = Search(index = \"papers*\", using = client)\n",
    "    query = {\"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\": {\"match_phrase\": {\"OriginalTitle\": paper[\"DBLP title\"]}}\n",
    "        }\n",
    "    }}\n",
    "\n",
    "    search.update_from_dict(query)\n",
    "\n",
    "    source_fields = ['PaperId',\"PaperTitle\", \"OriginalTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "    search = search.source(source_fields)\n",
    "\n",
    "    for res in search.scan():\n",
    "        if \"Year\" in res and res[\"Year\"]==paper[\"year\"] and res[\"OriginalTitle\"] == paper[\"DBLP title\"]:\n",
    "            paper[\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "            \n",
    "            paper[\"source\"] = \"ES\"\n",
    "\n",
    "    return paper\n",
    "    \n",
    "\n",
    "# def get_info_MAG(paper):\n",
    "    \n",
    "#     if \"source\" in paper:\n",
    "#         return paper\n",
    "    \n",
    "#     title = get_title_from_MAG(paper[\"DBLP title\"])\n",
    "    \n",
    "#     if title == None:\n",
    "#         return paper\n",
    "\n",
    "#     paper_data = evaluate_expr(title)\n",
    "    \n",
    "#     key_pairs = [\n",
    "#         (\"PaperTitle\",\"Ti\"),\n",
    "#         (\"PaperId\",\"Id\"),\n",
    "#         (\"Year\",\"Y\"),\n",
    "#         (\"CitationCount\",\"CC\"),\n",
    "#         (\"EstimatedCitation\", \"ECC\")\n",
    "#     ]\n",
    "    \n",
    "#     if \"entities\" not in paper_data:\n",
    "#         return paper\n",
    "    \n",
    "#     for entity in paper_data[\"entities\"]:\n",
    "#         info = {tokey: entity[fromkey] for tokey, fromkey in key_pairs}\n",
    "#         info[\"Affiliations\"] = [None if \"AfId\" not in author else author[\"AfId\"] for author in entity[\"AA\"]]\n",
    "#         info[\"Authors\"] = [None if \"AuId\" not in author else author[\"AuId\"] for author in entity[\"AA\"]]\n",
    "#         paper[\"MAG papers\"].append(info)\n",
    "    \n",
    "#     if len(paper[\"MAG papers\"]) > 0:\n",
    "#         paper[\"source\"] = \"MAG\"\n",
    "#         return paper\n",
    "    \n",
    "#     return paper\n",
    "\n",
    "    \n",
    "def get_paper_info(papers):    \n",
    "    \n",
    "    papers = get_info_es_canonical_title(papers)\n",
    "    \n",
    "    for paper in papers:\n",
    "#         if \"source\" not in paper:\n",
    "#             paper = get_info_es_display_title(paper)\n",
    "#         if \"source\" not in paper:\n",
    "#             paper = get_info_MAG(paper)\n",
    "        if \"source\" not in paper:\n",
    "            paper[\"source\"] = None\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "def get_paper_affiliations(papers):\n",
    "    \n",
    "    affiliations = set()\n",
    "    mag_keys = []\n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_keys.append(mag_paper[\"PaperId\"])\n",
    "    \n",
    "    batches = divide_batches(set(mag_keys),10)\n",
    "    paa_map = {k:{} for k in set(mag_keys)} \n",
    "    \n",
    "    for batch in batches:\n",
    "        search = Search(index = \"paperauthoraffiliations*\", using = client)\n",
    "        query = { \"query\": { \"terms\": {\"PaperId\": batch} } } \n",
    "        search.update_from_dict(query)\n",
    "        search = search.source(['PaperId','AuthorId','AffiliationId'])\n",
    "        res = [s.to_dict() for s in search.scan()]\n",
    "        \n",
    "        for pid in paa_map.keys():\n",
    "            matching_res = [r for r in res if pid == r[\"PaperId\"]]\n",
    "            for r in matching_res:\n",
    "                if r[\"AuthorId\"] not in paa_map[pid]:\n",
    "                    paa_map[pid][r[\"AuthorId\"]] = []\n",
    "                if \"AffiliationId\" in r:\n",
    "                    paa_map[pid][r[\"AuthorId\"]].append(r[\"AffiliationId\"])\n",
    "            \n",
    "\n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_paper[\"Affiliations\"] = dict()\n",
    "            for author, affs in paa_map[mag_paper[\"PaperId\"]].items():\n",
    "                num_affs = len(affs)\n",
    "                for aff in affs:\n",
    "                    if aff not in mag_paper[\"Affiliations\"]:\n",
    "                        mag_paper[\"Affiliations\"][aff] = 0\n",
    "                    mag_paper[\"Affiliations\"][aff] += 1.0/num_affs\n",
    "                    affiliations.add(aff)\n",
    "\n",
    "#     print(papers)\n",
    "    return papers, affiliations\n",
    "\n",
    "\n",
    "def divide_batches(list_like,n):\n",
    "    \n",
    "    list_like = list(list_like)\n",
    "    size = len(list_like)\n",
    "    \n",
    "    return [list_like[0+(n*x):min(n*(x+1),size)] for x in range(int(np.ceil(size/n)))]\n",
    "    \n",
    "\n",
    "def link_papers_with_affiliation_names(papers,affiliationids):\n",
    "\n",
    "    \n",
    "    aff_id_batches = divide_batches(affiliationids,100)\n",
    "    affiliations = dict()\n",
    "    \n",
    "    # get affiliation names\n",
    "    for batch in aff_id_batches:\n",
    "    \n",
    "        search = Search(index = \"affiliations\", using = client)\n",
    "        query = { \"query\": { \"terms\": {\"AffiliationId\": batch} } } \n",
    "        search.update_from_dict(query)\n",
    "        source_fields = ['AffiliationId',\"NormalizedName\"]\n",
    "        search = search.source(source_fields)\n",
    "        res = [r.to_dict() for r in search.scan()]\n",
    "        \n",
    "        for r in res:\n",
    "            affiliations[r[\"AffiliationId\"]] = r[\"NormalizedName\"]\n",
    "        \n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_paper[\"Affiliations\"] = {affiliations[aff_id]:count\n",
    "                                         for aff_id, count in mag_paper[\"Affiliations\"].items()}\n",
    "\n",
    "    \n",
    "    return papers\n",
    "    \n",
    "\n",
    "def get_information_for_venue_papers(venue, venuetype, yearrange=yearrange, force=False):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "    \n",
    "    \n",
    "    for year in yearrange:\n",
    "\n",
    "        in_filename = dblp_raw_filename(venue,year)\n",
    "        out_filename = filtered_papers_filename(venue,year)\n",
    "        \n",
    "        if not os.path.exists(in_filename):\n",
    "            print(in_filename, \"does not exist!\")\n",
    "            continue\n",
    "        \n",
    "        # check whether the file already exists\n",
    "        if os.path.exists(out_filename) and not force:\n",
    "            continue\n",
    "\n",
    "        with open(in_filename, \"r\") as fh:\n",
    "            papers = json.load(fh)\n",
    "\n",
    "        papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "\n",
    "        if len(papers) == 0:\n",
    "            with open(out_filename,\"w\") as fh:\n",
    "                json.dump([],fh)\n",
    "            continue\n",
    "        \n",
    "        affiliation_ids = set()\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for row in papers:\n",
    "\n",
    "            paper = dict()\n",
    "\n",
    "            paper[\"DBLP title\"] = row[\"title\"]\n",
    "            paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "            paper[\"year\"] = row[\"year\"]\n",
    "            paper[\"MAG papers\"] = list()\n",
    "            \n",
    "            output.append(paper)\n",
    "\n",
    "        output = get_paper_info(output)\n",
    "\n",
    "        output, paper_affiliations = get_paper_affiliations(output)\n",
    "\n",
    "        affiliation_ids.update(paper_affiliations)\n",
    "            \n",
    "        output = link_papers_with_affiliation_names(output, affiliation_ids)\n",
    "        \n",
    "        with open(out_filename,\"w\") as fh:\n",
    "            json.dump(output,fh)\n",
    "            \n",
    "    \n",
    "    print(venue)\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# get_information_for_venue_papers(\"iclr\", \"conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pools(task, lists, agg_f=None):\n",
    "    pool = Pool(processes = threads)\n",
    "    result = []\n",
    "    for x in lists:\n",
    "        result.append(pool.apply_async(task,(x,)))\n",
    "    \n",
    "    if agg_f is None:\n",
    "        def agg_f(x):\n",
    "            pass\n",
    "    for rs in result:\n",
    "        agg_f(rs.get())\n",
    "    pool.close()\n",
    "    \n",
    "    \n",
    "def popn(xs,n):\n",
    "    popped = list()\n",
    "    for i in range(n):\n",
    "        if len(xs) == 0:\n",
    "            break\n",
    "        popped.append(xs.pop())\n",
    "    return popped\n",
    "            \n",
    "def get_pool_lists(ls, threads):\n",
    "    ls_ = ls.copy()\n",
    "    if type(ls_) != list:\n",
    "        ls_ = list(ls_)\n",
    "    pool_lists = list()\n",
    "    list_size = len(ls) // threads\n",
    "    for i in range(threads-1):\n",
    "        pool_lists.append(popn(ls_,list_size))\n",
    "    pool_lists.append(ls_)\n",
    "    return pool_lists\n",
    "\n",
    "def task(venues):\n",
    "    \n",
    "    for venue, venuetype in venues:\n",
    "        get_information_for_venue_papers(venue, venuetype)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iclrtocsstacssoftwaresigcommsiamcomptalip\n",
      "poplwine\n",
      "pact\n",
      "\n",
      "microlats\n",
      "soda\n",
      "jeaiticseispairos\n",
      "siamconaaclinteract\n",
      "sea\n",
      "podswcciicws\n",
      "tocl\n",
      "tistsigsoft-fse\n",
      "\n",
      "rtas\n",
      "ssdbmtomccapicnp\n",
      "aistats\n",
      "\n",
      "jdiq\n",
      "infocom\n",
      "osdiisorc\n",
      "\n",
      "\n",
      "uist\n",
      "asiaccssigmod\n",
      "\n",
      "\n",
      "siamsctrets\n",
      "\n",
      "\n",
      "\n",
      "siamamitcwadsicstkr\n",
      "\n",
      "ssd\n",
      "\n",
      "tomacsdsnsdmpodcimctissec\n",
      "\n",
      "\n",
      "\n",
      "icmroopslamiccaitochi\n",
      "\n",
      "\n",
      "\n",
      "tpds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mobisys\n",
      "ubicomp\n",
      "siamadsvr\n",
      "rssjcdl\n",
      "ipsn\n",
      "\n",
      "\n",
      "ismvl\n",
      "\n",
      "\n",
      "\n",
      "popetstalgsiamrevtoiticmlsrdsijcar\n",
      "\n",
      "toisplditmm\n",
      "sigmetrics\n",
      "\n",
      "icsmtecshpcc\n",
      "mdm\n",
      "\n",
      "eurosys\n",
      "\n",
      "icciswcsc\n",
      "tosnjair\n",
      "\n",
      "uai\n",
      "recomb\n",
      "\n",
      "ipmiismmijcaiedccgeccokddtacl\n",
      "toistaco\n",
      "\n",
      "iclpddecs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hpca\n",
      "siamnum\n",
      "\n",
      "tosem\n",
      "icsereiiswcipdpsjacm\n",
      "mass\n",
      "ismb\n",
      "\n",
      "tdsc\n",
      "mobisecsbac-padissta\n",
      "cseicisjocch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raidsiammax\n",
      "\n",
      "\n",
      "tplp\n",
      "\n",
      "tos\n",
      "\n",
      "ipccc\n",
      "\n",
      "nsdi\n",
      "\n",
      "\n",
      "\n",
      "hotos\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iui\n",
      "\n",
      "\n",
      "ics\n",
      "eurocryptmascotssigirismarissretaccessieeemm3dimcolttccsaint\n",
      "cscwtmc\n",
      "asiacrypt\n",
      "\n",
      "si3d\n",
      "edbtpervasive\n",
      "togwisecpvldbdcossspinjmlricasspvldbfseicgsesiamma\n",
      "icebemobihoc\n",
      "www\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "toplas\n",
      "\n",
      "\n",
      "\n",
      "nomsioltshoti\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "twebitpro\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sigecomislpedissccesorics\n",
      "icwsmaaaitcbblics\n",
      "\n",
      "cryptotkde\n",
      "icrasacmatpercomsensyscolingecoopspaasoupspsb\n",
      "\n",
      "siamjojmicrotacasicapsdatewsdmtodsfocs\n",
      "\n",
      "icfpicdt\n",
      "asevee\n",
      "\n",
      "\n",
      "mobicom\n",
      "nipston\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "interspeech\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hipeac\n",
      "\n",
      "\n",
      "itng\n",
      "issac\n",
      "isca\n",
      "tvlsi\n",
      "sigcse\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cgaicprsp\n",
      "aamassactkddcptceccvsiamdmsecurecomm\n",
      "\n",
      "wowmomacsacpami\n",
      "ppoppfmcad\n",
      "clusterjetcicernfmicdmtomsesopdatamine\n",
      "mmasuss\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hipclctes\n",
      "\n",
      "ispass\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "internet\n",
      "icpp\n",
      "\n",
      "\n",
      "icalpsosp\n",
      "titb\n",
      "asapecai\n",
      "\n",
      "sec\n",
      "taaspacmpl\n",
      "conexticdeeurosp\n",
      "\n",
      "todaes\n",
      "rtss\n",
      "fccm\n",
      "cal\n",
      "\n",
      "jericccs\n",
      "tapndsswimob\n",
      "lcn\n",
      "tvcg\n",
      "\n",
      "grid\n",
      "\n",
      "\n",
      "daccomputer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm\n",
      "dt\n",
      "icdcs\n",
      "\n",
      "\n",
      "\n",
      "fast\n",
      "\n",
      "esacacm\n",
      "\n",
      "\n",
      "gis\n",
      "disctoctccgridicac\n",
      "usenix\n",
      "\n",
      "civriccvml\n",
      "\n",
      "\n",
      "acldigitelcvpr\n",
      "\n",
      "arith\n",
      "er\n",
      "\n",
      "sysose\n",
      "tslp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cikmcompsacemsofticccn\n",
      "cccg\n",
      "fase\n",
      "bpmacsac\n",
      "apsccstoc\n",
      "tse\n",
      "\n",
      "csuremnlp\n",
      "\n",
      "\n",
      "iccad\n",
      "expert\n",
      "\n",
      "iaaicompgeomaiannals\n",
      "\n",
      "chi\n",
      "\n",
      "\n",
      "csfw\n",
      "ccc\n",
      "\n",
      "\n",
      "\n",
      "aiccsa\n",
      "ancsbioinformaticscgocc\n",
      "\n",
      "\n",
      "ht\n",
      "\n",
      "aimcgfamai\n",
      "cavbibe\n",
      "\n",
      "\n",
      "hpdc\n",
      "\n",
      "\n",
      "casesaina\n",
      "avss\n",
      "\n",
      "ats\n",
      "asplos\n",
      "0.629686\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "venues = list()\n",
    "\n",
    "completed = list()\n",
    "\n",
    "with open(venue_category_filename, \"r\") as fh:\n",
    "    \n",
    "    reader = csv.reader(fh, delimiter=\",\")\n",
    "    \n",
    "    # skip header row\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        venue_type = row[4]   \n",
    "        name = row[0]\n",
    "    \n",
    "        venues.append((name, venue_type))\n",
    "\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to add additional filtered papers to a venue,year pair that has been affected by a change in the raw papers scraped or the filtering system without having to regather the information for the exisitng papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_for_venue_papers_add_additional_papers(venue, venuetype, year):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "\n",
    "    in_filename = dblp_raw_filename(venue,year)\n",
    "    out_filename = filtered_papers_filename(venue,year)\n",
    "    \n",
    "    \n",
    "    with open(in_filename, \"r\") as fh:\n",
    "        papers = json.load(fh)\n",
    "\n",
    "\n",
    "    papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "    \n",
    "    \n",
    "    with open(out_filename, \"r\") as fh:\n",
    "        output = json.load(fh)\n",
    "\n",
    "    original_output_size = len(output)\n",
    "    \n",
    "    existing_papers = [(paper[\"DBLP title\"],paper[\"year\"]) for paper in output]\n",
    "    \n",
    "    additional_papers = [paper for paper in papers if (paper[\"title\"],paper[\"year\"]) not in existing_papers]\n",
    "    \n",
    "    if len(additional_papers) == 0:\n",
    "        print(venue,year,\"nothing to add\")\n",
    "        return\n",
    "\n",
    "    affiliation_ids = set()\n",
    "    \n",
    "    additional_output = list()\n",
    "    \n",
    "    for row in additional_papers:\n",
    "\n",
    "        paper = dict()\n",
    "\n",
    "        paper[\"DBLP title\"] = row[\"title\"]\n",
    "        paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "        paper[\"year\"] = row[\"year\"]\n",
    "        paper[\"MAG papers\"] = list()\n",
    "\n",
    "        additional_output.append(paper)\n",
    "\n",
    "    additional_output = get_paper_info(additional_output)\n",
    "\n",
    "    additional_output, paper_affiliations = get_paper_affiliations(additional_output)\n",
    "\n",
    "    affiliation_ids.update(paper_affiliations)\n",
    "\n",
    "    additional_output = link_papers_with_affiliation_names(additional_output, affiliation_ids)\n",
    "\n",
    "    output.extend(additional_output)\n",
    "    \n",
    "    final_output_size = len(output)\n",
    "\n",
    "    with open(out_filename,\"w\") as fh:\n",
    "        json.dump(output,fh)\n",
    "        \n",
    "    print(venue,year,\"from\",original_output_size,\"to\",final_output_size)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wimob toittocs2007  2007tog nothing to addtognothing to addtkddtap  podc2007sigcse2007\n",
      " tomstoisjeric mobisys  nothing to add2013iticse  sigsoft-fse2015nothing to add\n",
      "\n",
      "nothing to add2014icmr  \n",
      "   2007nothing to addtois nothing to add\n",
      "tocl  \n",
      "tosn  2008scnothing to addsecurecomm\n",
      "2019201420072011 icistweb \n",
      "2007 mobihoc talip     2007 20082007  nothing to add    jacm2009nothing to add 2008nothing to add 20112007\n",
      "nothing to addnothing to add\n",
      "nothing to addnothing to addnothing to addnothing to add 20102007\n",
      "  \n",
      "\n",
      "\n",
      " 2007\n",
      " nothing to addnothing to add  nothing to addmobisystissecsigmetrics\n",
      " tochi\n",
      "2007nothing to add icmr \n",
      "  taconothing to add2017 sc  nothing to add2007nothing to addnothing to addjetc\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " tognothing to add20122007tosem2017 nothing to add  2007ismvltslp2009\n",
      "2007 tods2007iticse20072007  talg    \n",
      "nothing to add\n",
      " nothing to add  nothing to add2007nothing to add nothing to add icis nothing to add2007\n",
      "\n",
      " \n",
      "2011\n",
      " \n",
      "aamas  tognothing to add2007sc nothing to addtecs nothing to add saintnothing to add\n",
      " \n",
      "taas200920082007 2007nothing to add\n",
      "nothing to add   \n",
      " icppicmr\n",
      " toplas\n",
      "nothing to addtomccapnothing to addnothing to add nothing to addnothing to add 2012\n",
      "\n",
      " \n",
      "\n",
      "todaes 2011 2007    2007nothing to addmobisys\n",
      "nothing to add2007\n",
      "nothing to add \n",
      "  \n",
      " 200720072007 icissigcse2015ssdbm  nothing to add    nothing to addnothing to add2007nothing to addnothing to add\n",
      " nothing to add\n",
      "2007\n",
      "2008 \n",
      "\n",
      " 2018\n",
      " \n",
      "nothing to addnothing to add tomacs\n",
      "\n",
      "nothing to add nothing to add\n",
      "2007\n",
      "ancs  nothing to add2010\n",
      " nothing to add\n",
      "cgo 2014 nothing to add\n",
      "chi 2018 nothing to add\n",
      "chi 2019 nothing to add\n",
      "compgeom 2014 nothing to add\n",
      "conext 2007 nothing to add\n",
      "conext 2008 nothing to add\n",
      "conext 2010 nothing to add\n",
      "conext 2011 nothing to add\n",
      "csur 2007 nothing to add\n",
      "emsoft 2018 nothing to add\n",
      "fast 2012 nothing to add\n",
      "gis 2007 nothing to add\n",
      "gis 2008 nothing to add\n",
      "icac 2007 nothing to add\n",
      "iccad 2016 nothing to add\n",
      "iccad 2018 nothing to add\n",
      "icdcs 2007 nothing to add\n",
      "icis 2007 nothing to add\n",
      "0.435598\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "threads = 20\n",
    "\n",
    "def additional_papers_task(venues):\n",
    "    \n",
    "    for venue, venuetype, year in venues:\n",
    "        get_information_for_venue_papers_add_additional_papers(venue, venuetype, year)\n",
    "    \n",
    "    return None\n",
    "\n",
    "venues = list()\n",
    "\n",
    "with open(\"single_page_number_exclusions.csv\",\"r\") as fh:\n",
    "    reader = csv.reader(fh,delimiter=\",\")\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        year = int(row[1])\n",
    "        venue_type = row[-1]\n",
    "        venues.append((key,venue_type,year))\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(additional_papers_task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
