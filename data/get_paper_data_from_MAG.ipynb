{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests, sys, string, requests, csv\n",
    "import http.client, urllib.request, urllib.parse, urllib.error, base64\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from paper_filtering import filter_by_header\n",
    "from paper_filtering import filter_by_page_number_keep_missing\n",
    "from paper_filtering import filter_by_page_number_remove_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_keep_missing\n",
    "from paper_filtering import filter_by_header_and_page_number_remove_missing\n",
    "from paper_filtering import filter_journals\n",
    "from paper_filtering import apply_filter_to_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eshost = \"localhost:9200\"\n",
    "\n",
    "threads = 40\n",
    "\n",
    "client = Elasticsearch(eshost, timeout=200, maxthreads = threads)\n",
    "\n",
    "# years to include note: range(inclusive,exclusive, i.e. range(1,3) = [1,2]\n",
    "yearrange = range(2007,2022)\n",
    "\n",
    "# filepath to the file containing venue names and their categories (conf or journal)\n",
    "venue_category_filename = os.path.join(os.pardir,\"app\",\"data\",\"venue_list.csv\")\n",
    "\n",
    "\n",
    "dblp_raw_filename = lambda name,year: os.path.join( \"DBLP_raw_data\" , \n",
    "                                              \"{}_{}_raw_dblp_papers.json\".format(name,year) )\n",
    "\n",
    "filtered_papers_filename = lambda name,year: os.path.join( \"filtered_papers\" , \n",
    "                                              \"{}_{}_filtered_papers.json\".format(name,year) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = {\n",
    "    \"author\":\"authors_new\",\n",
    "    \"aff\":\"affiliations_new\",\n",
    "    \"paper\":\"papers_new\",\n",
    "    \"journal\":\"journals_new\",\n",
    "    \"fos_c\":\"fieldofstudychildren_new\",\n",
    "    \"conf_i\":\"conferenceinstances_new\",\n",
    "    \"conf_s\":\"conferenceseries_new\",\n",
    "    \"pref\":\"paperreferences_new\",\n",
    "    \"fos\":\"fieldsofstudy_new\",\n",
    "    \"pfos\":\"paperfieldsofstudy_new\",\n",
    "    \"paa\":\"paperauthoraffiliations_new\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLICATED (2021 June) -- search data from ES only (not from academic search API)\n",
    "#\n",
    "#\n",
    "# def query_academic_search(type, url, query):\n",
    "#     headers = {\n",
    "#         # Request headers\n",
    "#         'Ocp-Apim-Subscription-Key': '4698d5e7b0244e828d1dc21134238650',    # bens\n",
    "#     }\n",
    "#     if type == \"get\":\n",
    "#         response = requests.get(url, params=urllib.parse.urlencode(query), headers=headers)\n",
    "#     elif type == \"post\":\n",
    "#         response = requests.post(url, json=query, headers=headers)\n",
    "# #     if response.status_code != 200:\n",
    "# #         print(\"return statue: \" + str(response.status_code))\n",
    "# #         print(\"ERROR: problem with the request.\")\n",
    "# #         print(response.content)\n",
    "#         #exit()\n",
    "#     return json.loads((response.content).decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# def interpret_title(title):\n",
    "    \n",
    "        \n",
    "#     MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "#     url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/interpret\")\n",
    "#     query = {\n",
    "#       \"query\": title,\n",
    "#       \"count\": 1,\n",
    "#       \"offset\": 0,\n",
    "#       \"attributes\": \"Ti\"\n",
    "#     }\n",
    "\n",
    "#     data = query_academic_search(\"get\", url, query)\n",
    "#     expr = data[\"interpretations\"][0][\"rules\"][0][\"output\"][\"value\"]\n",
    "    \n",
    "#     return expr\n",
    "\n",
    "\n",
    "# def evaluate_expr(query):\n",
    "    \n",
    "    \n",
    "#     MAS_URL_PREFIX = \"https://api.labs.cognitive.microsoft.com\"\n",
    "#     url = os.path.join(MAS_URL_PREFIX, \"academic/v1.0/evaluate\")\n",
    "#     query = {\n",
    "#       \"expr\": \"Ti='{}'\".format(query),\n",
    "#       \"count\": 20,\n",
    "#       \"offset\": 0,\n",
    "#       \"attributes\": \"Id,Ti,Y,AA.AuId,AA.AfId,CC,ECC\"\n",
    "#     }\n",
    "\n",
    "#     data = query_academic_search(\"get\", url, query)\n",
    "    \n",
    "#     return data\n",
    "    \n",
    "    \n",
    "# def get_title_from_MAG(title):\n",
    "    \n",
    "#     try:\n",
    "#         expr = interpret_title(title)\n",
    "#         mag_title = expr[4:-1]\n",
    "        \n",
    "#     except:\n",
    "#         mag_title = None\n",
    "\n",
    "#     return mag_title\n",
    "\n",
    "\n",
    "def canonical(title):\n",
    "    title = title.lower()\n",
    "    title = title.translate(str.maketrans(punctuation, \" \"*len(punctuation)))\n",
    "    title = \" \".join(title.split())\n",
    "\n",
    "\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_es_canonical_title(papers):\n",
    "\n",
    "    year = papers[0][\"year\"]\n",
    "    \n",
    "    papers = {canonical(paper[\"DBLP title\"]):paper for paper in papers}\n",
    "        \n",
    "        \n",
    "    batches = divide_batches(set(papers.keys()),20)\n",
    "\n",
    "    \n",
    "    for batch in batches:\n",
    "        \n",
    "        canonical_titles = {canonical(title) for title in batch}\n",
    "\n",
    "        search = Search(index = index_name[\"paper\"], using = client)\n",
    "        query = {\"query\":{\n",
    "            \"bool\":{\n",
    "                \"should\": [{\"match_phrase\": {\"PaperTitle\": ct}} for ct in canonical_titles]\n",
    "            }\n",
    "        }}\n",
    "\n",
    "        search.update_from_dict(query)\n",
    "\n",
    "        source_fields = ['PaperId',\"PaperTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "        search = search.source(source_fields)\n",
    "\n",
    "        for res in search.scan():\n",
    "            if \"Year\" in res and res[\"Year\"] == year and res[\"PaperTitle\"] in canonical_titles:\n",
    "                ({field: res[field] for field in source_fields})\n",
    "\n",
    "                papers[res[\"PaperTitle\"]][\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "                papers[res[\"PaperTitle\"]][\"source\"] = \"ES\"\n",
    "\n",
    "    return list(papers.values())\n",
    "\n",
    "    \n",
    "def get_info_es_display_title(paper):\n",
    "    search = Search(index = index_name[\"paper\"], using = client)\n",
    "    query = {\"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\": {\"match_phrase\": {\"OriginalTitle\": paper[\"DBLP title\"]}}\n",
    "        }\n",
    "    }}\n",
    "\n",
    "    search.update_from_dict(query)\n",
    "\n",
    "    source_fields = ['PaperId',\"PaperTitle\", \"OriginalTitle\", \"Year\", \"CitationCount\", \"EstimatedCitation\"]\n",
    "\n",
    "    search = search.source(source_fields)\n",
    "\n",
    "    for res in search.scan():\n",
    "        if \"Year\" in res and res[\"Year\"]==paper[\"year\"] and res[\"OriginalTitle\"] == paper[\"DBLP title\"]:\n",
    "            paper[\"MAG papers\"].append({field: res[field] for field in source_fields})\n",
    "            \n",
    "            paper[\"source\"] = \"ES\"\n",
    "\n",
    "    return paper\n",
    "    \n",
    "\n",
    "# def get_info_MAG(paper):\n",
    "    \n",
    "#     if \"source\" in paper:\n",
    "#         return paper\n",
    "    \n",
    "#     title = get_title_from_MAG(paper[\"DBLP title\"])\n",
    "    \n",
    "#     if title == None:\n",
    "#         return paper\n",
    "\n",
    "#     paper_data = evaluate_expr(title)\n",
    "    \n",
    "#     key_pairs = [\n",
    "#         (\"PaperTitle\",\"Ti\"),\n",
    "#         (\"PaperId\",\"Id\"),\n",
    "#         (\"Year\",\"Y\"),\n",
    "#         (\"CitationCount\",\"CC\"),\n",
    "#         (\"EstimatedCitation\", \"ECC\")\n",
    "#     ]\n",
    "    \n",
    "#     if \"entities\" not in paper_data:\n",
    "#         return paper\n",
    "    \n",
    "#     for entity in paper_data[\"entities\"]:\n",
    "#         info = {tokey: entity[fromkey] for tokey, fromkey in key_pairs}\n",
    "#         info[\"Affiliations\"] = [None if \"AfId\" not in author else author[\"AfId\"] for author in entity[\"AA\"]]\n",
    "#         info[\"Authors\"] = [None if \"AuId\" not in author else author[\"AuId\"] for author in entity[\"AA\"]]\n",
    "#         paper[\"MAG papers\"].append(info)\n",
    "    \n",
    "#     if len(paper[\"MAG papers\"]) > 0:\n",
    "#         paper[\"source\"] = \"MAG\"\n",
    "#         return paper\n",
    "    \n",
    "#     return paper\n",
    "\n",
    "    \n",
    "def get_paper_info(papers):    \n",
    "    \n",
    "    papers = get_info_es_canonical_title(papers)\n",
    "    \n",
    "    for paper in papers:\n",
    "#         if \"source\" not in paper:\n",
    "#             paper = get_info_es_display_title(paper)\n",
    "#         if \"source\" not in paper:\n",
    "#             paper = get_info_MAG(paper)\n",
    "        if \"source\" not in paper:\n",
    "            paper[\"source\"] = None\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "def get_paper_affiliations(papers):\n",
    "    \n",
    "    affiliations = set()\n",
    "    mag_keys = []\n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_keys.append(mag_paper[\"PaperId\"])\n",
    "    \n",
    "    batches = divide_batches(set(mag_keys),10)\n",
    "    paa_map = {k:{} for k in set(mag_keys)} \n",
    "    \n",
    "    for batch in batches:\n",
    "        search = Search(index = index_name[\"paa\"], using = client)\n",
    "        query = { \"query\": { \"terms\": {\"PaperId\": batch} } } \n",
    "        search.update_from_dict(query)\n",
    "        search = search.source(['PaperId','AuthorId','AffiliationId'])\n",
    "        res = [s.to_dict() for s in search.scan()]\n",
    "        \n",
    "        for pid in paa_map.keys():\n",
    "            matching_res = [r for r in res if pid == r[\"PaperId\"]]\n",
    "            for r in matching_res:\n",
    "                if r[\"AuthorId\"] not in paa_map[pid]:\n",
    "                    paa_map[pid][r[\"AuthorId\"]] = []\n",
    "                if \"AffiliationId\" in r:\n",
    "                    paa_map[pid][r[\"AuthorId\"]].append(r[\"AffiliationId\"])\n",
    "            \n",
    "\n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_paper[\"Affiliations\"] = dict()\n",
    "            for author, affs in paa_map[mag_paper[\"PaperId\"]].items():\n",
    "                num_affs = len(affs)\n",
    "                for aff in affs:\n",
    "                    if aff not in mag_paper[\"Affiliations\"]:\n",
    "                        mag_paper[\"Affiliations\"][aff] = 0\n",
    "                    mag_paper[\"Affiliations\"][aff] += 1.0/num_affs\n",
    "                    affiliations.add(aff)\n",
    "\n",
    "#     print(papers)\n",
    "    return papers, affiliations\n",
    "\n",
    "\n",
    "def divide_batches(list_like,n):\n",
    "    \n",
    "    list_like = list(list_like)\n",
    "    size = len(list_like)\n",
    "    \n",
    "    return [list_like[0+(n*x):min(n*(x+1),size)] for x in range(int(np.ceil(size/n)))]\n",
    "    \n",
    "\n",
    "def link_papers_with_affiliation_names(papers,affiliationids):\n",
    "\n",
    "    \n",
    "    aff_id_batches = divide_batches(affiliationids,100)\n",
    "    affiliations = dict()\n",
    "    \n",
    "    # get affiliation names\n",
    "    for batch in aff_id_batches:\n",
    "    \n",
    "        search = Search(index = index_name[\"aff\"], using = client)\n",
    "        query = { \"query\": { \"terms\": {\"AffiliationId\": batch} } } \n",
    "        search.update_from_dict(query)\n",
    "        source_fields = ['AffiliationId',\"NormalizedName\"]\n",
    "        search = search.source(source_fields)\n",
    "        res = [r.to_dict() for r in search.scan()]\n",
    "        \n",
    "        for r in res:\n",
    "            affiliations[r[\"AffiliationId\"]] = r[\"NormalizedName\"]\n",
    "        \n",
    "    for paper in papers:\n",
    "        for mag_paper in paper[\"MAG papers\"]:\n",
    "            mag_paper[\"Affiliations\"] = {affiliations[aff_id]:count\n",
    "                                         for aff_id, count in mag_paper[\"Affiliations\"].items()}\n",
    "\n",
    "    \n",
    "    return papers\n",
    "    \n",
    "\n",
    "def get_information_for_venue_papers(venue, venuetype, yearrange=yearrange, force=False):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "    \n",
    "    \n",
    "    for year in yearrange:\n",
    "\n",
    "        in_filename = dblp_raw_filename(venue,year)\n",
    "        out_filename = filtered_papers_filename(venue,year)\n",
    "        \n",
    "        if not os.path.exists(in_filename):\n",
    "            print(in_filename, \"does not exist!\")\n",
    "            continue\n",
    "        \n",
    "        # check whether the file already exists\n",
    "        if os.path.exists(out_filename) and not force:\n",
    "            continue\n",
    "\n",
    "        with open(in_filename, \"r\") as fh:\n",
    "            papers = json.load(fh)\n",
    "\n",
    "        papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "\n",
    "        if len(papers) == 0:\n",
    "            with open(out_filename,\"w\") as fh:\n",
    "                json.dump([],fh)\n",
    "            continue\n",
    "        \n",
    "        affiliation_ids = set()\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for row in papers:\n",
    "\n",
    "            paper = dict()\n",
    "\n",
    "            paper[\"DBLP title\"] = row[\"title\"]\n",
    "            paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "            paper[\"year\"] = row[\"year\"]\n",
    "            paper[\"MAG papers\"] = list()\n",
    "            \n",
    "            output.append(paper)\n",
    "\n",
    "        output = get_paper_info(output)\n",
    "\n",
    "        output, paper_affiliations = get_paper_affiliations(output)\n",
    "\n",
    "        affiliation_ids.update(paper_affiliations)\n",
    "            \n",
    "        output = link_papers_with_affiliation_names(output, affiliation_ids)\n",
    "        \n",
    "        with open(out_filename,\"w\") as fh:\n",
    "            json.dump(output,fh)\n",
    "            \n",
    "    \n",
    "    print(venue)\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# get_information_for_venue_papers(\"iclr\", \"conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pools(task, lists, agg_f=None):\n",
    "    pool = Pool(processes = threads)\n",
    "    result = []\n",
    "    for x in lists:\n",
    "        result.append(pool.apply_async(task,(x,)))\n",
    "    \n",
    "    if agg_f is None:\n",
    "        def agg_f(x):\n",
    "            pass\n",
    "    for rs in result:\n",
    "        agg_f(rs.get())\n",
    "    pool.close()\n",
    "    \n",
    "    \n",
    "def popn(xs,n):\n",
    "    popped = list()\n",
    "    for i in range(n):\n",
    "        if len(xs) == 0:\n",
    "            break\n",
    "        popped.append(xs.pop())\n",
    "    return popped\n",
    "            \n",
    "def get_pool_lists(ls, threads):\n",
    "    ls_ = ls.copy()\n",
    "    if type(ls_) != list:\n",
    "        ls_ = list(ls_)\n",
    "    pool_lists = list()\n",
    "    list_size = len(ls) // threads\n",
    "    for i in range(threads-1):\n",
    "        pool_lists.append(popn(ls_,list_size))\n",
    "    pool_lists.append(ls_)\n",
    "    return pool_lists\n",
    "\n",
    "def task(venues):\n",
    "    \n",
    "    for venue, venuetype in venues:\n",
    "        get_information_for_venue_papers(venue, venuetype)\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmieurospuisttretstomccap\n",
      "tocsubicomp\n",
      "tistuai\n",
      "taliptwebstacssoftware\n",
      "sigcommtvlsisiamcomp\n",
      "tvcgrtassea\n",
      "\n",
      "popltslppact\n",
      "micronaacltse\n",
      "\n",
      "latsiticseirosispa\n",
      "interacticnpicws\n",
      "icebeicc\n",
      "\n",
      "hpccgecco\n",
      "\n",
      "\n",
      "edcceurosys\n",
      "\n",
      "ddecscsesiamscsdmmobisyspodscoltisorccga\n",
      "kr\n",
      "calssdbm\n",
      "\n",
      "3dim\n",
      "tissec\n",
      "\n",
      "\n",
      "tomacs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "dcoss\n",
      "\n",
      "aaai\n",
      "\n",
      "asiacrypthpca\n",
      "\n",
      "icst\n",
      "\n",
      "\n",
      "\n",
      "eurocrypt\n",
      "\n",
      "\n",
      "aamassiamrev\n",
      "ipsnccs\n",
      "ismvl\n",
      "mobisec\n",
      "\n",
      "colingcscwpodc\n",
      "cacmkdd\n",
      "icdt\n",
      "aclhotosasesc\n",
      "esoricsssd\n",
      "icasspdatetoitedbtbpm\n",
      "fse\n",
      "ai\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icsmrss\n",
      "\n",
      "osdi\n",
      "bioinformatics\n",
      "siamcoaiccsa\n",
      "icdm\n",
      "icmr\n",
      "\n",
      "siamnumismm\n",
      "tocl\n",
      "\n",
      "\n",
      "sodainfocomcluster\n",
      "\n",
      "\n",
      "bibe\n",
      "\n",
      "tpds\n",
      "aim\n",
      "mobihoc\n",
      "datamineicmltalghotipldi\n",
      "focsasapccgrid\n",
      "\n",
      "ipmi\n",
      "\n",
      "tecs\n",
      "crypto\n",
      "\n",
      "icderecombsbac-padtochi\n",
      "siamamoopsla\n",
      "\n",
      "\n",
      "\n",
      "jocchesopimc\n",
      "\n",
      "\n",
      "srds\n",
      "sigsoft-fse\n",
      "avsstosn\n",
      "\n",
      "\n",
      "aina\n",
      "\n",
      "icse\n",
      "\n",
      "tois\n",
      "iclpfmcad\n",
      "\n",
      "cccgarithcivrtaco\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hipeac\n",
      "\n",
      "cp\n",
      "iclrtmm\n",
      "re\n",
      "ipdps\n",
      "miccaisiamads\n",
      "tdscnsdi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "icapsdacatsijcar\n",
      "sigmodicdcs\n",
      "\n",
      "ecoop\n",
      "\n",
      "tosem\n",
      "icis\n",
      "itcwine\n",
      "fccm\n",
      "\n",
      "\n",
      "apscc\n",
      "\n",
      "ccc\n",
      "taccess\n",
      "\n",
      "icsismb\n",
      "\n",
      "aistatsconextcvpresa\n",
      "mobicom\n",
      "tmcpervasiveipcccraid\n",
      "\n",
      "si3dcikm\n",
      "spin\n",
      "hipcnomsmdm\n",
      "\n",
      "icalp\n",
      "\n",
      "asplos\n",
      "tog\n",
      "ijcai\n",
      "\n",
      "sigmetrics\n",
      "\n",
      "iccv\n",
      "\n",
      "toseccviswc\n",
      "saint\n",
      "icgsejea\n",
      "fastjmlr\n",
      "\n",
      "annalscc\n",
      "wcci\n",
      "\n",
      "tacas\n",
      "icra\n",
      "\n",
      "computer\n",
      "\n",
      "\n",
      "tkde\n",
      "tccsiammaxpvldb\n",
      "ismariolts\n",
      "sensys\n",
      "mmas\n",
      "\n",
      "percom\n",
      "massnips\n",
      "chi\n",
      "\n",
      "\n",
      "\n",
      "eriiswcspaaicac\n",
      "\n",
      "\n",
      "sigirgrid\n",
      "\n",
      "\n",
      "icfp\n",
      "\n",
      "csur\n",
      "\n",
      "sacmattodsfasedsn\n",
      "\n",
      "toplas\n",
      "ecai\n",
      "icccn\n",
      "cavancsjmicro\n",
      "wads\n",
      "\n",
      "taasisstacompsacjdiq\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tkddicprislpedsecurecomm\n",
      "\n",
      "psb\n",
      "\n",
      "mm\n",
      "mascotsinterspeech\n",
      "\n",
      "\n",
      "tcbbnfm\n",
      "\n",
      "pamisiammacgo\n",
      "\n",
      "\n",
      "spieeemm\n",
      "\n",
      "\n",
      "gisemsoftsigecom\n",
      "iaai\n",
      "\n",
      "\n",
      "icersac\n",
      "csfw\n",
      "\n",
      "popets\n",
      "iccadtodaesexperttondt\n",
      "cases\n",
      "vramai\n",
      "sysose\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "compgeom\n",
      "jcdlissretitbicppisca\n",
      "jetcsec\n",
      "ppopplics\n",
      "ml\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ndss\n",
      "cgf\n",
      "\n",
      "\n",
      "tcsiamjoicwsm\n",
      "emnlp\n",
      "sigcsepacmpl\n",
      "\n",
      "sosp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "htwisectoms\n",
      "\n",
      "rtsstoct\n",
      "\n",
      "discvldbinternet\n",
      "stoc\n",
      "\n",
      "\n",
      "lctes\n",
      "jairjeric\n",
      "\n",
      "\n",
      "isscc\n",
      "\n",
      "\n",
      "siamdm\n",
      "\n",
      "\n",
      "tap\n",
      "hpdc\n",
      "\n",
      "\n",
      "digitelveelcn\n",
      "\n",
      "\n",
      "soupsjacm\n",
      "\n",
      "\n",
      "\n",
      "ussiuiacsac\n",
      "\n",
      "\n",
      "usenixitpro\n",
      "issac\n",
      "\n",
      "\n",
      "itng\n",
      "ispass\n",
      "asiaccs\n",
      "tacl\n",
      "tplp\n",
      "www\n",
      "wsdm\n",
      "wowmom\n",
      "wimob\n",
      "27.319875\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "venues = list()\n",
    "\n",
    "completed = list()\n",
    "\n",
    "with open(venue_category_filename, \"r\") as fh:\n",
    "    \n",
    "    reader = csv.reader(fh, delimiter=\",\")\n",
    "    \n",
    "    # skip header row\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        venue_type = row[4]   \n",
    "        name = row[0]\n",
    "    \n",
    "        venues.append((name, venue_type))\n",
    "\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below can be used to add additional filtered papers to a venue,year pair that has been affected by a change in the raw papers scraped or the filtering system without having to regather the information for the exisitng papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_for_venue_papers_add_additional_papers(venue, venuetype, year):\n",
    "    \n",
    "    filter_f = filter_journals if venuetype == \"journal\" else filter_by_header_and_page_number_keep_missing\n",
    "\n",
    "    in_filename = dblp_raw_filename(venue,year)\n",
    "    out_filename = filtered_papers_filename(venue,year)\n",
    "    \n",
    "    \n",
    "    with open(in_filename, \"r\") as fh:\n",
    "        papers = json.load(fh)\n",
    "\n",
    "\n",
    "    papers, _ = apply_filter_to_papers(filter_f, papers, venue, year)\n",
    "    \n",
    "    \n",
    "    with open(out_filename, \"r\") as fh:\n",
    "        output = json.load(fh)\n",
    "\n",
    "    original_output_size = len(output)\n",
    "    \n",
    "    existing_papers = [(paper[\"DBLP title\"],paper[\"year\"]) for paper in output]\n",
    "    \n",
    "    additional_papers = [paper for paper in papers if (paper[\"title\"],paper[\"year\"]) not in existing_papers]\n",
    "    \n",
    "    if len(additional_papers) == 0:\n",
    "        print(venue,year,\"nothing to add\")\n",
    "        return\n",
    "\n",
    "    affiliation_ids = set()\n",
    "    \n",
    "    additional_output = list()\n",
    "    \n",
    "    for row in additional_papers:\n",
    "\n",
    "        paper = dict()\n",
    "\n",
    "        paper[\"DBLP title\"] = row[\"title\"]\n",
    "        paper[\"DBLP authors\"] = row[\"authors\"]\n",
    "        paper[\"year\"] = row[\"year\"]\n",
    "        paper[\"MAG papers\"] = list()\n",
    "\n",
    "        additional_output.append(paper)\n",
    "\n",
    "    additional_output = get_paper_info(additional_output)\n",
    "\n",
    "    additional_output, paper_affiliations = get_paper_affiliations(additional_output)\n",
    "\n",
    "    affiliation_ids.update(paper_affiliations)\n",
    "\n",
    "    additional_output = link_papers_with_affiliation_names(additional_output, affiliation_ids)\n",
    "\n",
    "    output.extend(additional_output)\n",
    "    \n",
    "    final_output_size = len(output)\n",
    "\n",
    "    with open(out_filename,\"w\") as fh:\n",
    "        json.dump(output,fh)\n",
    "        \n",
    "    print(venue,year,\"from\",original_output_size,\"to\",final_output_size)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomstosnwimob  toit20072007 tog nothing to addtog  nothing to addtocstkddtap \n",
      "\n",
      " 2007 2007tosemtomccap   2007 sigsoft-fse2007taco 2007 20112007nothing to add   nothing to addnothing to addnothing to add\n",
      "  nothing to add\n",
      "\n",
      "nothing to add\n",
      "sigcsetaliptweb2007nothing to addnothing to addtocl toplas \n",
      "\n",
      "  20072007 \n",
      "20072007tomacssc    todsnothing to add2007nothing to addnothing to addtog  podc\n",
      "\n",
      "nothing to add2007\n",
      "\n",
      "mobisystalg \n",
      "tochi nothing to add  jeric\n",
      "2007 icmriticsenothing to add 20092007 \n",
      "taas todaes 2009nothing to add tslp nothing to addnothing to add \n",
      "2014 \n",
      "  \n",
      "200720072007nothing to add nothing to addnothing to add\n",
      " \n",
      "nothing to addtogicmr\n",
      "\n",
      " ismvl    nothing to add  20072008ssdbm\n",
      " 20122007 2007  nothing to addnothing to add20072007 2014\n",
      "  nothing to add nothing to add \n",
      "nothing to addnothing to addicpp \n",
      "2008  icmr2007\n",
      "   2011nothing to add aamas\n",
      "nothing to add 2019 \n",
      "\n",
      "\n",
      " 2008nothing to add2015nothing to add nothing to addicisjacm\n",
      " \n",
      "tissec\n",
      " 2007nothing to add  \n",
      "mobisysnothing to addnothing to addmobihoc2007 2007 sigmetrics   nothing to add 2017 2010securecomm\n",
      "2011nothing to addnothing to add2017\n",
      "   \n",
      " nothing to add2008nothing to add\n",
      "sc  nothing to add \n",
      "iticsenothing to addnothing to add \n",
      "2007jetc\n",
      "\n",
      "nothing to add\n",
      "2011\n",
      "  nothing to add\n",
      "toismobisys\n",
      "saint sctecs   icisancs 2013200720152007 2012      nothing to addnothing to add20072009nothing to add\n",
      " sigcse\n",
      " nothing to add2010\n",
      "nothing to addnothing to add \n",
      " \n",
      " \n",
      "nothing to add2018tois\n",
      "nothing to add  2007cgonothing to add\n",
      " \n",
      "2014  nothing to addnothing to add\n",
      "\n",
      "icis 2008 nothing to add\n",
      "chi 2018 nothing to add\n",
      "chi 2019 nothing to add\n",
      "compgeom 2014 nothing to add\n",
      "conext 2007 nothing to add\n",
      "conext 2008 nothing to add\n",
      "conext 2010 nothing to add\n",
      "conext 2011 nothing to add\n",
      "csur 2007 nothing to add\n",
      "emsoft 2018 nothing to add\n",
      "fast 2012 nothing to add\n",
      "gis 2007 nothing to add\n",
      "gis 2008 nothing to add\n",
      "icac 2007 nothing to add\n",
      "iccad 2016 nothing to add\n",
      "iccad 2018 nothing to add\n",
      "icdcs 2007 nothing to add\n",
      "icis 2007 nothing to add\n",
      "0.329576\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "threads = 20\n",
    "\n",
    "def additional_papers_task(venues):\n",
    "    \n",
    "    for venue, venuetype, year in venues:\n",
    "        get_information_for_venue_papers_add_additional_papers(venue, venuetype, year)\n",
    "    \n",
    "    return None\n",
    "\n",
    "venues = list()\n",
    "\n",
    "with open(\"single_page_number_exclusions.csv\",\"r\") as fh:\n",
    "    reader = csv.reader(fh,delimiter=\",\")\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        year = int(row[1])\n",
    "        venue_type = row[-1]\n",
    "        venues.append((key,venue_type,year))\n",
    "        \n",
    "pool_lists = get_pool_lists(venues, threads)\n",
    "\n",
    "run_pools(additional_papers_task, pool_lists,)\n",
    "\n",
    "print((datetime.now()-start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
